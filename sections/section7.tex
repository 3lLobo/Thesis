
% TODO compare to molecules - point out differences

In this final chapter, we will discuss our results from different experiments, emphasizing the comparison to Simonovsky's work on molecule generation. The fact, that we did not achieve Similarly successful results does not hinder us from analyzing the responsible factors and causalities. Despite the nature of our results we are happy to propose possible solutions and while being of the opinion that asking the right question is more valuable than providing the correct answer, we will question the value of these solutions for further research.

\subsection{Experiment Take-away}

% Well well well
% We did link prediction because it is the most common experiment in this field

%  We see loss converges perfectly 
The first relevant observation made is the lower ELBO score to which the RGVAE converges when using $\beta=0$, meaning it is only trained on the reconstruction loss.
We tested the assumption that the model performs better without regularization term. This leads to the assumption that the RGVAE performs better better without prior constrain, yet a fully unconstrained VAE, defies the its purpose when it comes to disentangling the latent space and sampling.
The tuned RGVAE shows poor results on the task of link prediction, with slightly worse scores for the convolutional encoder. In order to find the isolate the causalities, we experiment further on link prediction with two variational version of DistMult learning on the full ELBO and only on the reconstruction loss. Both models scored equally bad compared to the original model, indicating that DistMult and embedding-based link predictors in general are not complex enough for variational inference. Following our prior assumption we added the unconstrained RGVAE to this comparison, which led to significantly higher scored than the all versions of the tuned RGVAE and the variation DistMult. Yet, the scores were not competitive with state of the art link predictors of with much lower parameter count, thus we do not recommend further researching in the direction of VAE as link predictor.

The graph matching loss function does not seem to have any impact on the link prediction results, yet we cannot draw a conclusion here since since the poor performance is linked to a different causality which might impede the graph matching from adding value. To measure the percentage of permutation we adopt Simonovsky's method of averaging the identity of permutation matrix $X$, which is invariant for permutation equivalent nodes. We see that during training the permutation rate converges to $60$\%.

Comparison of the two datasets, showed a better performance on the FB15K-237 dataset. While both datasets are real-world KGs, WN18RR is composed of synsets with vast discrepancy between the relation and entity count, resulting in a lower chance of generating a valid triple. Thus, we decided to continue the interpolation and graph generation experiments only on the FB15k-237 dataset. 

For the last two experiments we trained the RGVAE with $d_z=10$ for better disentanglement and analysis of each latent dimension.
The interpolation experiment between two triples reveals that the RGVAE in all versions fails to reconstruct the start and end triple. We notice an influence of the graph matching loss, namely the model predicting constantly the same relation versus the standard loss, where the model predicts randomly from a subset of triples. Same can be said for the predicted entities, which form a small subset of what seems to be the most frequent occurrences in the dataset. Empirically we can say that the prediction has little prior conditioning on the latent signal.

These last experiment is designed and evaluated in a similar fashion as Simonovsky did to generate valid molecular graphs. We interpolate the full latent space for $-\sigma^2 < z < \sigma$ and $-2\sigma^2 < z < 2\sigma$. Based on our proposed validation criteria, the RGVAE generated the same rate of valid triples as random. We observe further independence between the decoder and the latent signal. This slightly compares with Simonovsky's result where the generated valid triples had a variance of only $10$\%.
% Interpolation
%  like molecules


% Why does graph matching change that? 
% Higher dimension of freedom when generating graphs using Graph matching.

% All valid generated triples are unseen in both train and test set. 

% Simonovsky also reports low variance of valid triples 10\%

\subsection{Research Answer}

% Answer it right!

% How successful is a VAE in representation learning on real world KG compared to molecule graph data and what is the impact of each major hyperparameter?

%  Overall
To answer the initial research question we first summarize the influence of the main hyperparameter.
%  Convolutions
A convolutional encoder does not seem to contribute to the performance of the RGVAE. This could be caused by the simple implementation of using node and edge attributes both as feature matrix, or be due to the decoder issue, explained in section \ref{ssec7:collapse}, that the prediction is independent of the encoding. 
%  graph matching
Graph matching does have a noticeable impact. The model expresses more creative freedom when predicting the adjacency. Especially for experiments with larger subgraphs we recommend to keep investing in this method.

%  Prior, stochasticity
Variational inference is part of the VAE and allows disentanglement of the latent space, thus, even considering that the unconstrained model scored better, we recommend keeping the VAE structure. Instead of the full stochastic module, we assume the Standard Gaussian prior to be the reason behind the poor representation and recommend comparing alternative prior constrains.
Overall we can answer the research question:

\begin{center}
    \textit{The RGVAE failed in representation learning of real-world KG due to a decoder collapse.}
\end{center}

Yet, metaphorically, this is not the end of the book, but rather the beginning of a new chapter. In the remaining part of this discussion we analyze the underlying problem of the RGVAE and propose a variety of solutions.


\subsection{Decoder Collapse}
\label{ssec7:collapse}

%  what is decoder collapse
To introduce the main part of our conclusion we explain a phenomenon observed when training GANs. A GAN is yet another generative model which based on its potential to generate high resolution images, has drawn much attention in recent years. It consists of a generator, who's task it is to decode a latent signal to a an image, and a discriminator, which given the fake image and the real data has to distinguish between them. Training of GANs is unstable and one of the major problem which occur is \textit{mode collape}. The generator learns to fool the discriminator by generating only a single mode with high precision such that the discriminator classifies it was real image \cite{goodfellow2014generative}.

While VAEs cannot suffer from mode collapse, because they backpropagate over the predicted distributions of all modes, it has a related phenomenon. \textit{Decoder collapse} occurs when the decoder has enough capacity to choose not to consider the latent signal and instead stores the information to reconstruct the data's distribution partly or solely in its parameters. This means that the generated data is not conditioned on the latent signal anymore. The cause of this problem is found in the regularization term $D_{K L}\left(q_{{\phi}}\left(\mathbf{z} \mid \mathbf{x}\right) \| p_{{\theta}}(\mathbf{z})\right)$, more specifically in the constrain on a standard Gaussian prior. The multidimensional encoding of $d_z>1$ the approximated posterior is a mixture of Gaussians, which can only match the multivariate Normal distribution, in the case of all $\mathbf{\mu}_z=0$ and $\Sigma^2_z=\mathbb{I}$, what also implies that no information is encoded. Thus, the VAE has to decide if storing information in the latent vector is necessary to model the dataset distribution $p(x)$. If the penalty for altering the latent Normal distribution outweighs the benefits of the additional information towards the reconstruction loss, the VAE will chose for \textit{decoder collapse}. 

%  Cite https://towardsdatascience.com/with-great-power-comes-poor-latent-codes-representation-learning-in-vaes-pt-2-57403690e92b

% when we use a decoder with so much capacity that it chooses to not store information in the latent code at all, a result that leaves important information about our distribution locked up in decoder parameters, rather than neatly extracted as an internal representation.

% the network will only choose to make its z value informative if doing so is necessary to model the full data distribution, p(x). Otherwise, the penalty it suffers for using an informative z will typically outweigh the individual-image accuracy benefit it gets from using it.

% analyze results of interpolation and generation
Exactly this phenomenon can be observed in the triple interpolation and generation experiments. The RGVAE converges rapidly during training, minimizing the regularization loss close to zero and stabilizing the reconstruction loss by learning or matching the adjacency and predicting the most frequent node and edge attributes. This falsely indicates that the model has learned to reproduce the data. Yet, when sampling using only the decoder, the generated triples repeat combinations of a subset of entities and relations which are more frequent in the dataset. During interpolation of the latent space, the model predicts steadily the same relation. Subject and object seem to a certain degree be conditioned on the latent signal, altering while traversing the latent space, yet no clear disentanglement is observed. The final observation on the node attributes is, that the model shows a preference to predicts people's name and film's name entities as subject and location or music genre entities as object.

The last experiment confirms the obvious. The ratio of valid generated triples of the different model variations does not differ from random sampling. Further we again notice the predominance of the more frequent data entities and relations.


% For link prediction this means that the reconstruction loss is not helpful at all. 
Projecting this finding on the link prediction experiment, we question, why the model scored better than the random baseline. Possible reasons are for once, that the encoder learned to encode the dataset close to a Normal distribution, but when encountering unseen triples, the mean and variance of the encoding vary. A second reason is, that every triple in the testset is corrupted in all possible combinations, including the less frequent entities. The collapsed decoder subsequently keeps generating frequent triples, thus the reconstruction loss between a less frequent combination of entities and the collapsed prediction will be higher than for a frequent combination. Therefore the model learns score the real triple is higher but for the wrong reason. The RGVAE with $\beta = 0$ and therefore unconstrained on the Standard Gaussian prior shows the best results between the different model settings. We can assume that the model learns a latent representation for subject and object. Despite this improvement, is it probable that the decoder still collapses on the reconstruction of the relation index, which explains the remaining scoring gap to the much simpler DistMult model.


\subsection{VAE surgery}
\label{ssec7:solutions}
% Propose solutions: Elbosurgery, adversial(WAE),  recurrent(lossy)

Similar problems in different fields have been encountered for generative VAE applications. While the author of this thesis whishes to have drawn these parallels earlier, all the proposed solutions imply a significant modification of the VanillaVAE, thus would not align with this work's research question. We present three approaches from different literature which tackle the VAE mode collapse in the filed of image and voice generation.

%  ELBO surgery
In a publication of Adobe Research and Google Brain, Hoffman \textit{et al.} propose an elegant modification of the variational evidence lower bound \cite{hoffman2016elbo}. 
More specifically they change the VanillaVAE's regularization term, where instead of imposing a Standard Gaussian prior on the full latent distribution, the prior is imposed on the mixture of all single latent dimension, giving space for more expressive probability distributions such as truncated Gaussians. 
Further, a index-code mutual information term is added, intended to maximize the mutual information between every index of the observation and $z$. While the reconstruction term enforces to encode every feature of $x$ in a corresponding latent dimension, the  information term opposes this by maximizing the mutual information between all  $x_i$ and $z_i$. The information term compares compact to the reconstruction loss, yet it is enough to prevent decoder collapse. 


% Lossy Auto Encoder
Kingma \textit{et al.} turn towards a autoregressive solution in their publication \cite{chen_variational_2017}. Their VAE model generates images recursive per pixel, each conditioned on previous point, using both RNN and RCN as decoder. Their solution to the decoder collapse is a normalizing flow, which predicts the encoder posterior $q_{\phi}(z \mid x)$. Besides not suffering from decoder collapse, this model can be set to discard irrelevant information in the data. As downside, the autoregressive nature causes slow image generation.


% Wasserstein or AAE
The last approach we present closes the circle to the introduction of this section. The paper \textit{Wasserstein Auto-Encoders} \cite{tolstikhin_wasserstein_2019} propose a combination between GAN and VAE. A model wich uses the VAE's encoder-decoder architecture but instead of the normal regularization term, the posterior distribution is learned and penalized by its Wasserstein distance to the data distribution. This is in fact a generalization of the adversarial loss of generator and discriminator. Since our task of generating triples would benefit from a precise prediction, such as GANs achieve on image generation, we question, if employing adversarial loss also on the reconstruction term would yield better generation results in this field than the exact index-wise loss?

% Two different papers, same principle:

% Use GAN loss for VAE latent space distribution. Generator produces a distribution and discriminator tys to tell if its fake or true.

% Similar approach with Wasserstein distance as regularization loss. 


% Could we also usefully employ adversarial loss on the reconstruction part of the network (that is: have a discriminator try to tell apart input and reconstruction), to get away from the over-focus on exact detail reconstruction that comes with pixel-wise loss


% Remaining: Amortized Inference Regulation and Skip Connections 
Besides these three, numerous other approaches have been proposed. Worth mentioning because of their originality are the idea of adding skip-connections between latent space and the decoder's hidden layer \cite{dieng_avoiding_2019} as well as regularizing the amortized inference \cite{shu_amortized_2019}.  

% delta-VAE <--

% Regulation of the amortized inference.

% Skip connections between latent space and hidden layer of the decoder.



% Two layerVAE

\subsection{Future Work}

There are many avenues to follow for future work. Obviously the first step is to fix the collapsing decoder. The suggested solutions should be compared before making a choice, since none has yet been shown to work on KG VAEs.

Once the reconstruction of triples proved successful, we should look at the data, and question the representation. The adjacency matrix can be inferred from the edge attribute matrix and thus, is obsolete. Since it does not contain additional information, does the model perform worse without it? Approaching this thought from a different angle, we could leave the adjacency as it is and represent the node and edge indices as continuous number, as it is done for color scales of images. This would greatly reduce the number of parameters, but certainly also imply new challenges. Important in this context is that the adjacency is represented in sparse format since this the experiments on triples is but a proof of concept for larger subgraphs. 

The model's mayor hurdle seems to be predicting the correct relation index. While interpolating the entire latent space, less than $10$ different relations were generated. A further challenge is the varying count and position of the edges, in contrast to the number of nodes which is constant. Thus, we wonder if a setup of two VAEs, where the first one predicts the adjacency and node attributes and the second recurrent one, while conditioned on the predicted edges of the first VAE, predicts the edge attribute.

Finally and making use of the batch wise implementation of the max-pooling graph matching alorithm presented in this work, we suggest to explore a more efficient and \textit{intelligent} graph matching approach. Would it be possible drastically reduce complexity by training a neural network to predict the optimal permutation matrix based on target and prediction graph?
% with one of the presented solutions, starting with the simplest.

% Is the adjacency matrix even necessary? 

% Basing on the believe that further research will be fruitful, we recommend:

% \begin{itemize}
%     \item Smarter approach for graph matching, let a NN learn the best permutation given the targe and prediction
%     \item Two VAEs, first one-shot only the adjacency, second recurrent edgewise including entity and relation index.
%     \item Disentangle the latent space and condition on text
%     \item NF prior
% \end{itemize}

Hopefully, enough food for though has been provided for everyone willing to continue research in this field and look with excitement towards future findings.

% \textbf{Happy New Year!}
