This section describes the methods used in the experiments of this thesis. We will begin with data formatting and preprocessing, then the presentation the model, mainly the encoder and decoder and their variations. Moving on we explain our implementation of graph matching the loss function of the model and its evaluation metrics. To end of this chapter we describe the link prediction pipeline which is the main experiment to evaluate our model. All experiments are aimed to be fully reproducible and the model meant to be used in future work, thus the code is openly available on Github \footnote{\url{https://github.com/INDElab/rgvae}}.

\subsection{Knowledge graph data}
All our presented methods will operate on KG data. While data from other graph domains is possible, this work focuses solely on datasets in triple format. We will explain the sparse graph representation, which is the input format for our model and how to preprocess the original KG triples to match that format.

\subsubsection{Sparse representation}

% The first step in our pipeline is the representation of the KG in tensor format. In order to represent the graph structure we use an adjacency matrix $A$ of shape $n\times n$ with $n$ being the number of nodes in our graph. The edge attribute or directed relations between the nodes are represented in the matrix $E$ of shape $n\times n\times d_E$ with $d_E$ being the number of edge attributes. Similarly for node attributes we have the matrix $F$ of shape $n\times d_N$ with $d_N$ number of node attributes. The input graph can have less nodes than the maximum $n$ but not more. The diagonal of the adjacency matrix is filled with $1$ if the indexed node exists, and with $0$ otherwise. The number and encoding of the attributes must be predefined and cannot be changed after training. This way we can uniquely represent a KG.

In this work, we opt for the sparse graph representation $G(A,E,F)$, where $A$ denotes the adjacency matrix, $E$ the edge feature matrix and $F$ the node feature matrix. This allows models architectures as discussed in \ref{ssec:GVAE}. The graph is binary and each matrix is stored in a separate tensor.

% adjacency matrix
The adjacency matrix $A$ takes the shape $n\times n$ with $n$ being the number of nodes in our graph/subgraph. While most of previous work would only allow edges on the upper triangular adjacency matrix and fill the diagonal with ones, we chose a less constrained representation, which we assume is a better fit for representing KGs. In particular, we allow self-loops, meaning a triple where object and subject are the same entity and our relations are directed and can be inverted. Thus $A$ can have a positive signal at any position $A_{i,j}$  $i,j \in \mathbb{R}^{n \times n}$, indicating a directed edge between node of index $i$ and node of index $j$, while $A_{i,j}$ differs from $A_{j,i}$.

% edge attribute matrix
The edge attribute matrix $E$ takes the shape $n\times n\times n_e$ with $n_e$ being the number of unique entities in our dataset. For each possible edge in the adjacency matrix we have a one hot encoded vector pointing to the unique relation in the dataset. Stacking these vectors leads to the three dimensional matrix $E$.

% node attribute matrix
The shape of node attributes matrix $F$ is $n\times n_e$ with $n_e$ being the number of node attributes describing the nodes. Considering that we will split the KG in subgraphs, we use the entity index as node attribute, making it possible to assign every node in a subgraph to the entity in the full KG. Thus, the number of node attributes $n_e$ equals the number unique entities in our dataset. Again the node attributes are one hot encoded vectors, which stacked result in the two dimensional $F$ matrix.


\subsubsection{Preprocessing}
Our datasets consist of three tab separated value files full of triples for training, evaluation and final prediction.The preprocessing steps do not only account for generating subgraphs in the right format but also to ensure valuable research by withhold the triples in the test set until the final run.

% set of entities, set of relations 
% triple to graph
% all triples
% true triples dict
% indexing
From all three sets, we create a set of all occurring entities and similar set for the relations. Now we can define our dimensions $n_e$ and $n_r$. For both sets we create two dictionaries \textit{index-2-entity} and \textit{entity-2-index}  which map back and forth between numerical index and the string representation of the entity (similar for the relation set). These dictionaries are used to create a train and test set of triples with numeric indices. Depending on if we are in the final testing stage or not we include all triples from the training and evaluation file in the training set and use the triples in the testing file as test set, or we ignore the triples in the test file and use the evaluation file triples as test set.

%  truetriples dict 
Further we create two dictionaries, \textit{head} and \textit{tail} which for all occurring subject and relation combination, contain all entities which would complete it to a real triple in our dataset (similar for all relation and object combinations). This will allows us to filter true triples, an important part of link prediction and helpful in graph generation.  

%  triple 2 graph
The final step of preprocessing is a function, which takes a batch of numerical triples and converts them to a batch of binary, multidimensional tensors $A$, $E$ and $F$. While this might sound easy for only one triple per graph, it proves more complex for graphs with $n>2$ facing exemption cases such as self loops or an entity occurring in two triples. We solve this by creating a separate set for head and tail entities, then storing the indices od both in a list, starting with the subject set and finally using this list as keys for a dictionaries with values in the range to $n$. In both edge cases, this results in an adjacency matrix with a rank lower than $n$. A similar approach, with less edge cases to consider, is used to apply the inverted translation from tensor matrix to triple.

% Graph embeddings? unsupervised approach

\subsection{RGVAE}
The principle of a graph VAE has been explained in \ref{ssec:GVAE}, what also covers the foundation of our model, the Relational Graph VAE (RGVAE). Therefore we will focus on the implementation as well as parameter and hyperparameter choice. Since this work is meant to be a prove of concept rather than aimed at outperforming the state of the art, our model is kept as simple as possible and only as complex as necessary. Our appr
For the encoder we implemented two variations, a fully connected and a convolutional, while for the decoder we opted for a single fully connected network.
%  just explain the code

\subsubsection{Initialization}

The RGVAE is initialized with a set of hyperparameter, which define the input shape. Table \ref{tab:RGVAE} shows a complete list of those parameters and their default values. It is left to mention that we use the Xavier uniform method with a gain of $0,01$ to initialize the weight parameter.

\begin{table}[H]
\centering
    \begin{tabular}{|l|l|l|}
    \hline
    \rowcolor[HTML]{EFEFEF}
    \multicolumn{1}{|c}{\textsc{Hyerp.}} & \multicolumn{1}{c}{\textsc{Default}} & \multicolumn{1}{c|}{\textsc{Description}} \\\hline
    $n$     & \multicolumn{1}{c|}{$2$} & Number of nodes  \\
    $n_e$   &\multicolumn{1}{c|}{-}   & Total number of entities\\
    $n_r$   &\multicolumn{1}{c|}{-} & Total number of relations\\
    $d_z$ &\multicolumn{1}{c|}{$100$}   & Latent space dimension\\
    $d_h$ &\multicolumn{1}{c|}{$1024$}   & Hidden dimension\\
    $dropout$ &\multicolumn{1}{c|}{$0.2$}   & Dropout\\
    $\beta$ & \multicolumn{1}{c|}{$1$}  & $\beta$ value for regularization  \\
    $perminv$ & \multicolumn{1}{c|}{\textbf{True}}  & Permutation invariant loss function  \\
    $clipgrad$ & \multicolumn{1}{c|}{\textbf{True}}  & Learning w/ gradient clipping  \\
    $encoder$ & \multicolumn{1}{c|}{\textbf{MLP}}  & Learning w/ gradient clipping  \\
    \hline
    \end{tabular}
\label{tab:RGVAE}
\caption{The inital hyperparameter of the RGVAE with default value and description.}
\end{table}


\subsubsection{Encoder}
% \\Convolution part
% \\RCGN relation Convolution neural net
% \\MLP encoder
% \\Latent space
% \\reparametrization trick
% \\MLP decoder
% \\Graph matching
% \\Discretization of prediction

% MLP
The prove-of-concept encoder is a MLP as described in \ref{ssec:mlp}, which takes the flattened concatenated threefold graph $x=G(A,E,F)$ as batch input. We use the initial parameters to calculate the input

\begin{equation}
\label{eq4:inputdim}
    d_{input} = n*n + n*n*n_r + n*n_e
\end{equation}

The main encoder architecture is a 3 layer fully connected network, with both layers using ReLU as as activation function. The choice for two hidden layers is based on the huge difference between $d_{input}$ and $d_z$. The first layer has a dimension of $2*d_h$ and the option to use dropout, which by default is set to $0.2$. The second (hidden) layer has the dimension $d_h$ which is by default set to $1024$. After the second ReLU activation, the encoder linearly transforms the hidden state to an output vector of $2 \times d_z$. This vector is split and makes the mean and log-variance of size $d_z$ for the reparametrization trick. Sampling $\epsilon$ from an autonomous module, we get the latent representation $z$ of $x$,  the final output of the encoder.

% % Convolutions
The second option for our RGVAE encoder is a GCN as described in \ref{ssec:gcn}. We adopt the architecture from \cite{kipf_semi-supervised_2017} namely two layers of graph convolutions with dropout in between. To match the encoder output to the base model, we then add a flattening and a final linear transformation layer and to substitute the feature matrix used in Kipf's work, we reduce the edge attribute matrix $E$ by one dimension and concatenate it with $F$ resulting in $x_{GCN} \in \mathbb{R}^{n \times (d_e+n*d_r)}$. We forward pass the adjacency matrix $A$ and $x_{GCN}$ through the first GCN layer with a hidden dimension of $d_h$ and ReLU as activation function, followed by a dropout layer. It should be mentioned that dropout is only applied during learning, not on evaluation. The second GCN layer takes the hidden state and again $A$ as input the two dimensional output from the previous layer. Now, instead of having the GCN predict on a number of classes, we have it output a logits vector of dimension $2*d_z$. Therefore we pass the GCN output through a flattening and a linear transformation layer. Similar to above described encoder we use the reparametrization trick to output the latent reparametrization $z$.  


%  TODO:
Table comparing both architectures!!!


\subsubsection{Decoder}


For our RGVAE decoder, we use the same minimal approach as in \cite{simonovsky_graphvae_2018}, namely an MLP with inverted dimensions. The decoder architecture is similar to the one described in  COMPARING TABLE for the MLP encoder version. Since we are decoding the latent space, the input dimension is $d_z$ and the output dimension is $d_{input}$ as calculated in equation \ref{eq4:inputdim}. The flat logits output tensor is split threefold and reshaped to the original input shape of $G(A,E,F)$.   



To sample from the generated graph we apply the Sigmoid activation function to the logits of the first matrix and use the normalized output as weights for binomial distributions, from which we can sample the discrete $\tilde{A}$. For $\tilde{E}$ and $\tilde{F}$ we take the argmax on the last dimension of both matrices, Each node and edge can have only one attribute, referring to its index in $\mathcal{E}$ and $\mathcal{V}$, thus only the highest predicted value is relevant. The generated sample is a discrete graph $\tilde{G}(\tilde{A},\tilde{E},\tilde{F})$.


\subsubsection{Model Limitations}

The main limitation of the RGVAE is the parabolic increase of model parameters with the increase of nodes per input graph $\mathcal{O}(n^2)$. The number of parameters to train is directly linked with the GPU memory requirements. Even more computationally expensive is the use of permutation invariant graph matching, with a complexity of $\mathcal{O}(n^3)$. Thus, we propose this model only for generating small graphs with $n<30$.


% The proposed model is expected to be useful
% only for generating small graphs. This is due to growth
% of GPU memory requirements and number of parameters
% (O(k
% 2
% )) as well as matching complexity (O(k
% 4
% )), with small
% decrease in quality for high values of k. I
% \cite{simonovsky_graphvae_2018}


% \subsubsection{Hyperparameters}
% learning rate
% beta for regularization term
% hidden dimensions
% latent dimensions
% dropout
% n, n_e, n_r
% Convolutions vs no convolutions
% IDEA: Convolutions + MLP


\subsection{RGVAE learning}
% Lay out pipeline.
GPU requirements.
LISA
Nvidia titan RX 25GB 60h
Experiment log wndb.ai [cite]
optimizer ranger github repo link

\subsubsection{Max pooling graph matching}

%  explain the code
% batch implementation
% loop implementation to check
Summing over the neighbors means summing over the whole column
Normalize matrix with Frobenius Norm

Batch version:
Only matmul and dot. keep dimension of S with shape (bs,n,n,k,k)
When maxpooling, flatten Xs (n,k) for batch dot multiplication. This way (i think) we sum over all j nad b neighbors instead of taking the max.  

\subsubsection{Loss function}

% If loss function should be permutation invariant we need to do some kind of graph matching.\\
% Different options for graph matching.\\
% Maxpooling-algorithm:\\
% Assumptions\\
% Node to edge affinity equals 0\\

% Self-loops are possible, adjacency matrix can be zero or one.\\


In contrast to his implementation we assume, that a node or edge can have none, one or multiple attributes. Therefore our attributes are also not sigmoided and do not sum up to one. This leads o the modification of term logF and logE where we do not matrix multiply over the attribute vector but take the BCE as over the rest of the points.
KG can have multiple or no attributes vs molecular graphs can be one hot encoded.

Okay, further we need to treat the $log_pE$ and $log_pF$ just like $log_pA$ and subtract the inverse. Otherwise the model learn to predict very high values only. 

A note to the node features, these stay softmaxed and one-hot encoded since we will use them as node labels.

beta VAE

Hungarian algorithm for discrimination of X


The hungarian algorithm as presented in section 2 return the shortest path in a matrix. We use this shortest path as bast match between the two graphs. The node paris identified as optimal are masked as $1$ and the rest of the matrix as $0$. This way we discretizice $X\star$ to $X$. 


\subsection{Link prediction and Metrics}


% First:
% Link prediction to make a proof of concept, not achieve SOTA.\\
% Node classifier by only using encoder.\\
% latent space interpolation to find analogies to smile vector in the latent space of a face VAEs.\\
% Identify if VAE learns semantics. OWL, onthology datasetbatch_size, required.\\
% Wasserstein distance???
% Link prediction?
% blabl


\begin{itemize}
    \item Load Dataset
    \item Convert to sparse in batches
    \item forward pass through VAE
    \item MPGM loss
    \item Backward pass
    \item test on val set
    \item calculate MRR on subset of val set
    \item draw graphs
\end{itemize}


\subsection{Variational DistMult}

Control model
reparametrization trick in middle
still same scoring function

