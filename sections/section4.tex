This section describes the methodology for this thesis. The first part includes the presentation of the model, the reprocessing of the input and the evaluation metrics. The second part describes the experimental setup and the different experimental runs. The work of this thesis has aimed to be fully reproducable, thus the code is opensourced and available on Github \footnote{***Thesis Repo Link***}.

\subsection{Knowledge graph representation}

% adjacency matrix
% edge attribute matrix
% node attribute matrix
The first step in our pipeline is the representation of the KG in tensor format. In order to represent the graph structure we use an adjacency matrix $A$ of shape $n\times n$ with $n$ being the number of nodes in our graph. The edge attribute or directed relations between the nodes are represented in the matrix $E$ of shape $n\times n\times d_E$ with $d_E$ being the number of edge attributes. Similarly for node attributes we have the matrix $F$ of shape $n\times d_N$ with $d_N$ number of node attributes. The input graph can have less nodes than the maximum $n$ but not more. The diagonal of the adjacency matrix is filled with $1$ if the indexed node exists, and with $0$ otherwise. The number and encoding of the attributes must be predefined and cannot be changed after training. This way we can uniquely represent a KG.
\\


Graph embeddings? unsupervised approach

\subsection{Graph VAE}
hi
\\Convolution part
\\RCGN relation Convolution neural net
\\MLP encoder
\\Latent space
\\reparametrization trick
\\MLP decoder
\\Graph matching
\\Discretization of prediction

\subsection{Loss function}

If loss function should be permutation invariant we need to do some kind of graph matching.\\
Different options for graph matching.\\
Maxpooling-algorithm:\\
Assumptions\\
Node to edge affinity equals 0\\

Self-loops are possible, adjacency matrix can be zero or one.\\

Summing over the neighbors means summing over the whole column
Normalize matrix with Frobenius Norm

Batch version:
Only matmul and dot. keep dimension of S with shape (bs,n,n,k,k)
When maxpooling, flatten Xs (n,k) for batch dot multiplication. This way (i think) we sum over all j nad b neighbors instead of taking the max.  


Hungarian algorithm for discrimination of X \\
\\
The hungarian algorithm as presented in section 2 return the shortest path in a matrix. We use this shortest path as bast match between the two graphs. The node paris identified as optimal are masked as $1$ and the rest of the matrix as $0$. This way we discretizice $X\star$ to $X$. 
\\
\textbf{The equations}

\subsection{{Hyperparameters}
learning rate
beta for regularization term
hidden dimensions
latent dimensions
Convolutions vs no convolutions
IDEA: Convolutions + MLP

\subsection{Experiments and Metrics}


First:
Link prediction to make a proof of concept, not achieve SOTA.\\
Node classifier by only using encoder.\\
latent space interpolation to find analogies to smile vector in the latent space of a face VAEs.\\
Identify if VAE learns semantics. OWL, onthology datasetbatch_size, required.\\
Wasserstein distance???
Link prediction?
blabl

\subsection{{Datasets & Preproceccing}
We used two popular datasets in this field to test our model and compare to State of the art (SOTA) results.

\subsubsection{Fb15k-237}
This dataset is a subset of the FreeBase KG (cite).[Explain Free Base] The original Fb15k had redundant triples, creating ambiguity during training. In the updated version, Fb15k-237, these triples were removed what led to a more robust dataset, which is commonly used as benchmark for several NLP tasks.

% or/and
A subset from on of the first and largest KGs, called FreeBase. In the first version of this dataset, it was possible to infer most of the test triples by inverting triples in the trainset, Thus the latest $237$ version filtered these triples out.
The dataset contains $14,951$ entities and $1,345$ different relations.

\subsubsection{WN18rr}
Based on the wordnet KG.

\subsubsection{Data Preprocessing}
Converting from dense to sparse.

Learning histogram for link prediction.

\subsection{Model Training}
Lay out pipeline.
\begin{itemize}
    \item Load Dataset
    \item Convert to sparse in batches
    \item forward pass through VAE
    \item MPGM loss
    \item Backward pass
    \item test on val set
    \item calculate MRR on subset of val set
    \item draw graphs
\end{itemize}

GPU requirements.
Nvidia titan RX 25GB 60h
