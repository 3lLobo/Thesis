% Experiments

This section presents the experiments we ran. We covered link and node prediction and compared those to SOTA scores. Further we ran experiments on investigating the coherence of the reproduced graph structure. Lastly we measured the adherence of our model to the KG's underlying syntax.

\subsection{Datasets and Training}
For this sake of meaningful results, we chose to use the earlier presented, two most popular dataset used in this field, FB15k237 and WN18rr.

Training models on each dataset for 333 epochs, without early stopping.

\subsection{Link Prediction}
% The task of link prediction, tests, if the model can recognize the relation between two entities. In order to do so, we hide the second entity of a triple and let our model to predict it.

We used link prediction as evaluation protocol and for comparison state of the art models. For each triple in the dataset, we remove the tail and combine it with all possible entities in our dataset. EVen though this is called 'triple-corruption', also correct triples can be generated, which could appear in the trainset. These have bo the filtered out before evaluation. Link prediction on unfiltered test data is termed 'raw'. Our model then computes the ELBO loss for all corrupted triples, which are sorted and stored in ascending order. The same procedure is repeated the triple's head in place of the tail.


The metrics used to evaluate the predictions, is the mean reciprocal rank (MRR) and hits at 1,3 and 10.
The MRR ??? explain what the MRR is. \linebreak
Hits at 1 indicates what percentage of the triples in the test set have been ranked the highest compared to their corruptions. Similar, hits at 3 and 10, give a percentage of triples ranked in the top 3 and top 10.
These metrics allow a fair comparison on a dataset between models, regardless of the ranking method of each model.



% Evaluation protocol For evaluation, we use the same ranking procedure as in [3]. For each test
% triplet, the head is removed and replaced by each of the entities of the dictionary in turn. Dissimilarities (or energies) of those corrupted triplets are first computed by the models and then sorted by
% ascending order; the rank of the correct entity is finally stored. This whole procedure is repeated
% while removing the tail instead of the head. We report the mean of those predicted ranks and the
% hits@10, i.e. the proportion of correct entities ranked in the top 10.
% These metrics are indicative but can be flawed when some corrupted triplets end up being valid
% ones, from the training set for instance. In this case, those may be ranked above the test triplet, but
% this should not be counted as an error because both triplets are true. To avoid such a misleading
% behavior, we propose to remove from the list of corrupted triplets all the triplets that appear either in
% the training, validation or test set (except the test triplet of interest). This ensures that all corrupted
% triplets do not belong to the data set. In the following, we report mean ranks and hits@10 according
% to both settings: the original (possibly flawed) one is termed raw, while w

\subsubsection{RGVAE hyperparameter tuning}

RGVAE:
hidden dimensions
latent dimensions
beta


RGCVAE

\subsubsection{RGVAE lp}
Different models
Report:
loss converges
MRR stays almost zero

With vs. w/o graph matching


\subsubsection{DistMult lp}
To find out why our model fails to perform lp we run the original DistMult with optimal parameters from \cite{ruffinelli_you_2019} and a variational DistMult version, which samples from a latent distribution, just like the VAE. Give implementation link.

Answer question:
We see that the variational part messes everything up.

Graphs:
MRR + Hits@all + Loss

\subsection{Impact of permutation}
% Check if adj matrix adheres to edge attribute matrix.

Graph of permutation during training
loss with vs without 

\subsection{Interpolate Latent Space}
We take two random triples and interpolate the latent space of these two triples. The interpolations result in: HERE AN EXAMPLE.

Further we go ahead and test what happens if we modify one latent dimension of a triple. HERE AN EXAPMPLE.

Can the model assign logical features to latent dimensions?


\subsection{Syntax coherence}

Check model trained with vs without perm invariance.

Check if generated triples follow basic logic.
\begin{itemize}
    \item Generate triples by random signals
    \item Filter these triples on a certain relation
    \item Check if the entities are part of the linked class
\end{itemize}

Since there is no preset for how to check the semantics of a KG, we will use simple basic logical criteria.
The generated triples are filtered for the relation 'is capital of', thus the subject entity should be a city and he object entity member of the class 'country',
Hope this gives good results.

\subsection{Subgraph Generation}
Until now our model trained on only one triple per sparse graph. What will happen if we train it on more than one triples?
