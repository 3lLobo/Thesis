This section presents previous work which inspired and laid the foundation for this thesis. Relevant publication on topics related to this thesis are presented in terms of methods and results. We focus on the fields of relational graph convolutions, graph encoders, and embeddin- based link prediction.

\subsection{Relational Graph Convolutions}
We define a graph as $G=(\mathcal{V}, \mathcal{E})$  with a set of nodes $\mathcal{V}$ and a set of edges $\mathcal{E}$. The set of edges, with each edge connecting node $x$ and $y$, is defined by $\left\{(x, y) \mid(x, y) \in \mathcal{V}^{2} \wedge x \neq y\right\}$ while the constraint $x \neq y$ prohibits self-connections or self-loops, which is optional depending on the graphs function. Moreover, nodes and edges can have features, which contribute additional information about the nodes and their connection. In the literature, these features can be describing attributes and properties, in the context of this work we will also use them as indicators to unique entities. Graph convolutions, make use of both these properties and the spectral information in a graphs adjacency matrix. In graph theory spectral properties are the characteristic polynomial, eigenvalues, and eigenvectors of the Laplacian and adjacency matrix \cite{chung1997spectral}. Two popular tasks to evaluate the performance of a neural network on graphs, are node classification and link prediction. The first is a classification problem where the model predicts the class of a node. Link prediction is the task of completing a triple by correctly predicting the missing entity at either head or tail the triple. A in-depth explanation follows in section \ref{ssec4:lpmetrics}.


% Present the relational graph convolution model paper by Kipf and maybe others
In Kipf \textit{et al.} paper on graph convolutions \cite{kipf_semi-supervised_2017} a novel Graph Convolution Network (GCN) for semi-supervised classification is introduced. The model takes as input the adjacency matrix and optionally a feature matrix of the graph and predicts the classes of the nodes. Graph convolutions acts directly on the graph structure and are linearly scalable with the number of nodes. The GCN takes as input the adjacency matrix $A \in \mathbb{R}^{n \times n}$ with $n$ being the number of nodes in the graph. In the case of undirected graphs, the adjacency matrix is symmetric. The output is a matrix $H \in \mathbb{R}^{n \times d_h}$ where $d_h$ are the hidden dimensions or in case od the last layer, the number of classes to predict over. 
While the authors compare different propagation methods for the graph convolutions, their propagating rule using a first-order approximation of spectral graph convolutions, outperforms all other implementations. Propagation denotes the transformation of the input data between layers of a model. Kipf approximates the eigenvalues of the Laplacian with first order Chebyshev polynomials and circumvents the computationally expensive Eigendecomposition. The renormalization trick normalized the adjacency matrix and adds it to an identity matrix of same size. This keeps the eigenvalues in a range between $[0,2]$ which again leads to a stable training, avoiding numerical instabilities and vanishing gradients during learning. Additionally the feature information of neighboring nodes is propagated in every layer what shows improvement in comparison to earlier methods, where only label information is aggregated.
Kipf and Welling perform node classification on the three citation-network datasets, Citeseer, Cora and Pubmed as well as on the KG dataset NELL. In all classification tasks, their results outperform other recently proposed methods in this field and proves to be computationally more efficient than its competition. For more details on the implementation of graph convolutions we refer to section \ref{ssec:gcn}. 

% Kipfs second paper 
In their publication \textit{Modeling Relational Data with Graph Convolutional Networks} Schlichtkrull \textit{et al.} propose a relational graph convolutional network (RGCN) and evaluate it on link prediction on the FB15K-237 and WN18 dataset and node classification on the AIFB, MUTAG, BGS and AM datasets \cite{gangemi_modeling_2018}. The RGCN, with its encoder properties, is used by itself as node classifier, yet for link prediction it is coupled with a DistMult model acting as decoder which scores triples encoded by the RGCN see figure \ref{fig:RGCN}. We go into details of the embedding-based DistMult model in section \ref{ssec:embedlp}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.55\textwidth]{data/images/RGCN.png}
    \caption{RGCN with encoder-only for node classification and encoder-decoder architecture for link prediction experiments. Source \cite{gangemi_modeling_2018}.}
    \label{fig:RGCN}
\end{figure}

The RGCN works on dense graphs stored as triples, creating a hidden state for each node. A novel message passing network is layer-wise propagated with the hidden states of the entities. As regularization the authors propose a \textit{basis-} and \textit{blockwise} decomposition. while the first  aims at an effective weight sharing between different relation types, the second  can be seen as a sparsity constraint on the relation type's weight. The model outperforms embedding based model on the link prediction task on the FB15K-237 dataset and scores competitive on the WN18 dataset. In the node classification task, the model sets state of the art results on the datasets AIFB and AM, while scoring competitive on the remaining. The authors conclude, that the model has difficulties encoding higher-degree hub nodes on datasets with many entities and low amount of classes. This is noticeable as it relates to the WN18RR \cite{battaglia_relational_2018}, one of the two datasets used in this thesis.  


\subsection{Graph VAE}
% Present different papers with graph VAEs
We have seen how graph convolutional neural networks can be combined in an encoder-decoder architecture, resulting in a generative model suitable for unsupervised learning. We present three recent publications with different methods and use cases of a graph generative model, in particular a VAE.

% Kipfs VGAE
Kipf \textit{et al.} introduce the the Variational Graph Autoencoder (VGAE), a framework for unsupervised learning on graph-structured data \cite{kipf_variational_2016}. This generative model uses a GCN as encoder and a simple inner product module as decoder. Similar to the GCN, the VGAE incorporates node features, which significantly improves its performance on link prediction tasks compared to related models. The VGAE uses a two-layer GCN to encode the mean and the logvariance for the stochastic module to sample the latent space representation, more specifically, a latent vector per node. Reffering to the above described GCN, the VGAE encoder outputs a latent matrix $H \in \mathbb{R}^{n \times 2d_z}$ with $d_z$ denoting the latent dimension. The activation of the inner product of this latent matrix yields the reconstruction of the adjacency matrix. Figure \ref{fig:kipfGVAE} shows how the model learns to cluster  nodes according to their class, without these labels being provided to the model during training.
This visualization shows that the VGAE learns successful learn an implicit representation of the data.
The VGAE with added features outperforms state of the art methods (to the time of publication) Spectral Clustering \cite{tang2011leveraging} and Deepwalk \cite{perozzi2014deepwalk} in the task of link prediction on the datasets Cora, Citeseer and Pubmed. The authors point out, that a Gaussian prior might be a poor choice combined with the inner-product decoder.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{data/images/KipGVAE.jpg}
    \caption{Visualization of the VGAE's latent representation of the Core citation network. Colors express the disentanglement of node classes. Source \cite{kipf_variational_2016}}
    \label{fig:kipfGVAE}
\end{figure}

% TODO I think it's important to state the difference between GVAE and GraphVAE very clear (that is the GVAE learns a latent vector per node and the GraphVAE learn a lv for the whole graph).


Simonovsky \textit{et al.} introduce the GraphVAE, which generates a probabilistic fully-connected molecule graph of a predefined maximum size
in a one-shot approach \cite{simonovsky_graphvae_2018}. In this context fully-connected denotes that all nodes are connected within a graph, in contrast to citation networks where subgraphs can be disconnected from each other. While molecule graphs have a lower node and edge count than citation networks, their edges and nodes are attributed, which constrains each connection. The model includes a standard graph matching algorithm, which finds the optimal permutation between the predicted graph and the ground truth and. The reconstruction loss considers the permutation instead of the raw prediction. In contrast to the previously presented publications, the input to this model is a threefold and sparse graph, defined as $G=(A, E, F)$ with $A$ being the adjacency matrix, $E$ the edge attribute matrix and $F$ the node attribute matrix, with $E$ and $F$ being one-hot encoded. Considering that this method lays the foundation for this thesis, we adopt this notation for our own methods in section \ref{sec:mthods}. Figure \ref{fig:graphvaefull} shows the architecture of the GraphVAE. The encoder is a feed forward network with edge-conditioned graph convolutions \cite{simonovsky2017dynamic}, which takes as input the target graph $G$ with $n$ nodes. After the convolutions the result is flattened and conditioned on the node labels $y$. A fully-connected neural network encodes the stochastic latent representation, which is constrained by Standard Gaussian prior distribution. Note that in contrast to the GCN, which encodes one latent vecrot per nodem, the GraphVAE instead encodes a latent representation of the whole graph. This latent representaion is again conditioned on the node labels $y$ and propagated through the decoder in form of a fully-connected neural network. The decoder reconstructs the latent representation to the graph prediction. The threefold decoder output is matched with the target using graph matching algorithm, which we discuss further in section \ref{ssec:graphmatch}. The matched and permuted graph is then used for the reconstruction term of the GraphVAE loss. It should be noted, that, while the size of the target and prediction graph are fixed, they do not necessarily have to match. While this approach seems promising, it is limited by the maximum graph size, which has been experimented with up to a node count of $40$.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{data/images/GraphVAEfull.png}
    \caption{Model architecture of the GraphVAE. Source \cite{simonovsky_graphvae_2018}.}
    \label{fig:graphvaefull}
\end{figure}

The model is trained on the QM9 dataset, containing the graph structure of 134k organic molecules with experiments on latent space dimension in the range of $[20,80]$. On the free generation task, about $50\%$ of the generated molecules are chemically valid and thereof remarkably $60\%$ are not included in the training dataset. When testing the model for robustness, it showed little disturbance when adding Gaussian noise to the input graph $G$. The authors conclude that the problem of generating graphs
from a continuous embedding was addressed successfully and that the GraphVAE performs better on small molecules, implying a low node count per graph.


Until here the presented models generate graphs in a single propagation through the model. For completeness we also present a successful approach for graph generation in autoregressive manner. Belli \textit{et al.} introduce named approach for image-conditioned graph generation of road network graphs \cite{belli_image-conditioned_2019}. While we focus on the generative model, their contribution ranges wider, namely the introduction of the graph-based roadmap dataset \textit{Toulouse Road Network} and the task specific distance metric \textit{StreetMover}. The authors propose the Generative Graph Transformer (GGT) a deep autoregressive model that makes use of attention mechanisms on images, to tackle the challenging task of road network extraction from image data. The GGT has a encoder-decoder architecture, with a CNN as encoder, taking the grayscale image as input signal and predicting a conditioning vector. The decoder is a self-attentive transformer, which takes as input the encoded condition vector and a hidden representation of the adjacency matrix $A$ and feature vector $X$ of the previous step. The adjacency matrix here indicates the links between steps and the features are normalized coordinates. A multi-head operator outputs the hidden representation of $A$ and $X$ which finally are decoded by a MLP to the graph representation. For the first step, a empty hidden representation is fed into the decoder. The model terminates the recurrent graph generation by predicting a end-of-sequence token, which signalizes the end of the graph. During learning, the generated graphs are matched to the target graphs using the \textit{StreetMover} metric, based on the Sinkhorn distance. The authors attribute \textit{StreetMover} as a scalable, efficient and permutation-invariant metric for graph comparison. The successful results of the experiments performed, show that this novel approach is suitable for the task of road network extraction and could yield similar success in graph generation task of different fields.
While this publication does not directly align with the previously presented work, we find it of added value to present alternative approaches on our topic.

% \begin{itemize}
%     \item Belli recurrent VAE
%     \item GraphVAE paper
%     \item Variational Graph Auto-Encoders
% \end{itemize}


\subsection{Embedding-Based Link Prediction}
\label{ssec:embedlp}
Finalizing this chapter, we will look at embedding-based methods on KGs.
Compared to the previously presented research, embedding models have a much simpler architecture and can be trained computationally very efficient on large graphs. Embedding-based models can only operate on triples, meaning a KG is represented as a set of triples with indices pointing to the unique entity and relation in the graph. Despite their simplicity, they achieve great results on node and link prediction tasks.

Already in 2013 Bordes \textit{et al.} introduced in their paper \textit{Translating Embeddings for Modeling Multi-relational Data} the low-dimensional embedding model TransE \cite{bordes_translating_2013}. The core idea of this model is that relations can be represented as translations in the embedding space. Entities are encoded to a low-dimension embedding space and the relation is represented as vector between the head and tail entity. The assumption is that for correct triples the model learns to reduce the Euclidean distance between head and tail entity by placing them closer together in the embedding space. This results in correct triples having a lower norm of the relational vector than corrupted triples. Using this property, the model can predict the missing entity in link prediction.

The models loss function takes a set of corrupted triples for every triples in the training set and subtracts the translation vector of the corrupted triple in embedding space from the translation vector of the correct triple with added margin. To minimize the loss, the model has to place entities of correct triples closer together in embedding space. We think of a triple as $(s,r,o)$ and $(e_s,e_r,e_o)$ as its embedded representation, $d()$ the Euclidean distance, $\gamma$ the positive margin and $S$ and $S^{\prime}$ as sets of correct and corrupt triples, the loss function of TransE is 

\begin{equation}
    \mathcal{L}=\sum_{S} \sum_{S^{\prime}} \left[\gamma+d(\boldsymbol{s}+\boldsymbol{r}, \boldsymbol{o})-d\left(\boldsymbol{s}^{\prime}+\boldsymbol{r}, \boldsymbol{o}^{\prime}\right)\right].
\end{equation}

The model is trained on a subset of the KGs Freebase and Wordnet, which is also the source for the datasets used in this thesis. TransE's link prediction results on both head and tail outperformed other competing methods of the time, such as RESCAL \cite{nickel_three-way_nodate}.

In 2015, Yang \textit{et al.} proposed a similar, yet better performing KG embedding method \cite{yang_embedding_2015}. Their model DistMult captures relational semantics by matrix multiplication of the embedded entity representation and uses a bilinear learning objective. The main difference to TransE is the bilinear scoring function $d^{b}()$. Bilinearity is indicated by the exponent $b$ and connotes the functions score-invariance of swapping the triples head and tail entity. For the embedding space representation of subject and object $e_s$ and $e_{o}$ and a diagonal matrix $\operatorname{diag}(e_{r})$ with the embedded relation $e_r$ on the diagonal, the scoring function is

\begin{equation}
    d^{b}\left((e_s,e_r,e_o)\right)=e_s \operatorname{diag}(e_{r}) e_o.
    \label{eq2:distmult}
\end{equation}

The publication goes on to explore the options of embedding-based rule extraction from KGs. Concluding, the authors state that the prediction scores achieved with the embeddings learned from the bilinear objective not only outperform the state of the art in link prediction but can also capture compositional semantics of relations and extract Horn rules using compositional reasoning.

In a more recent publication, Ruffinelli \textit{et al.} present a comprehensive review of KG embedding models such as TransE and DistMult, coupled with state of the art techniques in deep learning. The authors start by pointing out the similarities and differences of most models. While all methods share the same embedding approach, they differ in their scoring function and their original hyperparameter search. The authors perform a quasi-random hyperparameter search on the five models RESCAL, TransE, DistMult, ComplEx and ConvE, which each use a characteristically different loss function. They are compared by their MRR and Hits@$10$ scores on the two datasets FB15K-237 and WN18. Since these metrics and datasets are used later on in our research, they are explained in section \ref{ssec5:data}(datasets) and \ref{ssec4:lpmetrics}(metrics). The tuned models report a higher MRR score of up to $24\%$ compared to their first reported performance. The authors conclude that simple KG embedding methods can show strong performance when trained with state of the art techniques what indicates that higher complexity is not necessary. The optimal model configurations, which were found by a random search of the hyperparameter space, are included in this publication. 

% Graph Embeddings\\
% TransE represents entities in in low-dimensional embedding. The relationships between entities are represented by the vector between two entities \cite{bordes_translating_2013}.
% (How are different relation between the same entities represented?)

% OntoUSP\\
% This method learns a hierarchical structure to better represent the relations between entities in embedding space.
