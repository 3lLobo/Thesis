% !BIB program = bibtex
\documentclass{article}

\usepackage{graphicx, color}

\usepackage[a4paper,margin=2cm]{geometry}

% Here are my package imports:
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{layouts}
\usepackage[table,xcdraw]{xcolor}

\setlength{\parindent}{3em}
\setlength{\parskip}{1em}
\newcommand{\red}[1]{{\color{red}{#1}}}

\RequirePackage[backend=bibtex,style=nature]{biblatex}

\addbibresource{fullrefs.bib}

\begin{document}




\begin{titlepage}



\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page

 

%----------------------------------------------------------------------------------------

%	HEADING SECTIONS

%----------------------------------------------------------------------------------------



\includegraphics[width=\linewidth]{data/images/uvaENG}\\[2.5cm]

\textsc{\Large MSc Artificial Intelligence}\\[0.2cm]

\textsc{\Large Master Thesis}\\[0.5cm] 



%----------------------------------------------------------------------------------------

%	TITLE SECTION

%----------------------------------------------------------------------------------------



\HRule \\[0.4cm]

{ \huge \bfseries Knowledge Generation \\[0.4cm] } % Title of your document

\HRule \\[0.5cm]

 

%----------------------------------------------------------------------------------------

%	AUTHOR SECTION

%----------------------------------------------------------------------------------------



by\\[0.2cm]

\textsc{\Large Florian Wolf}\\[0.2cm] %you name

{12393339}\\[1cm]





%----------------------------------------------------------------------------------------

%	DATE SECTION

%----------------------------------------------------------------------------------------



{\Large \today}\\[1cm] % Date, change the \today to a set date if you want to be precise



{48 Credits}\\ %
{April 2020 - December 2020}\\[1cm]
%{Period in which the research was carried out}\\[1cm]%



%----------------------------------------------------------------------------------------

%	COMMITTEE SECTION

%----------------------------------------------------------------------------------------

\begin{minipage}[t]{0.4\textwidth}

\begin{flushleft} \large

\emph{Supervisor:} \\

Dr Peter \textsc{Bloem} \\ Thiviyan \textsc{Singam} \\ Chiara \textsc{Spruijt} % Supervisor's Name

\end{flushleft}

\end{minipage}

~

\begin{minipage}[t]{0.4\textwidth}

\begin{flushright} \large

\emph{Asessor:} \\

Dr Paul \textsc{Groth}\\

\end{flushright}

\end{minipage}\\[2cm]



%----------------------------------------------------------------------------------------

%	LOGO SECTION

%----------------------------------------------------------------------------------------



% \framebox{\rule{0pt}{2.5cm}\rule{2.5cm}{0pt}}\\[0.5cm]

\includegraphics[width=2.5cm]{data/images/uva.png}\\ % Include a department/university logo - this will require the graphicx package

% \textsc{\large \red{institute name}}\\[1.0cm] % 

 

%----------------------------------------------------------------------------------------



\vfill % Fill the rest of the page with whitespace



\end{titlepage}

\tableofcontents
\listoftables
\newpage

\section*{Abstract}
% We generate Knowledge! \cite{kipf_contrastive_2020}

This thesis investigates the idea of having a VAE learn latent features of the raw data from KGs. Building on successful approaches of prior work, an embedding based, a fully-connected and a convolutional model are evaluated on the two most popular KGs. The influence of a permutation invariant loss function on the models capability to generalize on sparse subgraphs is analyzed.
We compare the performance of our models on the task of link prediction to sate of the art scores.
The results compare ???
The model $x$ outperforms the others, indicating that convolutions are [/not] necessary.
Interpolation of the latent space shows that the model learns to featurize latent dimensions???
When generating subgraphs with up to $n$ nodes, we see ???
Finally we filter generated triples for type constrained predicates on subject or object. A $x\%$ of the generated triples adhere to this axiom. Thus we can say that VAE's are to a certain extend able to capture the underlying semantics of a KG.   


% \section*{Notes}
% \input{notes}

\section{Introduction}
\input{sections/section1}

\section{Related Work}
\input{sections/section2}

\section{Background}
\input{sections/section3}

\section{Methods}
\input{sections/section4}
\label{sec:mthods}

\section{Experiments \& Results}
\input{sections/section5}

% \section{Results}
% \input{sections/section6}

\section{Discussion \& Future Work}
\input{sections/section7}
\label{sec:discus}

% \section*{Ideas}
% \input{ideas}

% \section*{Open Issues}
% \input{issues}

\printbibliography
\end{document}

% \bibliographystyle{unsrt} % We choose the "plain" reference style
% \bibliography{fullrefs} % Entries are in the "refs.bib" file

