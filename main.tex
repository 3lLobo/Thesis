% !BIB program = bibtex
\documentclass{article}

\usepackage{graphicx, color}

\usepackage[a4paper,margin=2cm]{geometry}

% Here are my package imports:
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algorithm} 
\usepackage{algpseudocode} 

\setlength{\parindent}{3em}
\setlength{\parskip}{1em}
\newcommand{\red}[1]{{\color{red}{#1}}}

\RequirePackage[backend=bibtex,style=nature]{biblatex}

\addbibresource{fullrefs.bib}

\begin{document}





\begin{titlepage}



\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page

 

%----------------------------------------------------------------------------------------

%	HEADING SECTIONS

%----------------------------------------------------------------------------------------



\includegraphics[width=\linewidth]{data/images/uvaENG}\\[2.5cm]

\textsc{\Large MSc Artificial Intelligence}\\[0.2cm]

\textsc{\Large Master Thesis}\\[0.5cm] 



%----------------------------------------------------------------------------------------

%	TITLE SECTION

%----------------------------------------------------------------------------------------



\HRule \\[0.4cm]

{ \huge \bfseries Knowledge Generation \\[0.4cm] } % Title of your document

\HRule \\[0.5cm]

 

%----------------------------------------------------------------------------------------

%	AUTHOR SECTION

%----------------------------------------------------------------------------------------



by\\[0.2cm]

\textsc{\Large Florian Wolf}\\[0.2cm] %you name

{12393339}\\[1cm]





%----------------------------------------------------------------------------------------

%	DATE SECTION

%----------------------------------------------------------------------------------------



{\Large \today}\\[1cm] % Date, change the \today to a set date if you want to be precise



{48 Credits}\\ %
{April 2020 - December 2020}\\[1cm]
%{Period in which the research was carried out}\\[1cm]%



%----------------------------------------------------------------------------------------

%	COMMITTEE SECTION

%----------------------------------------------------------------------------------------

\begin{minipage}[t]{0.4\textwidth}

\begin{flushleft} \large

\emph{Supervisor:} \\

Dr Peter \textsc{Bloem} \\ Chiara \textsc{Spruijt} % Supervisor's Name

\end{flushleft}

\end{minipage}

~

\begin{minipage}[t]{0.4\textwidth}

\begin{flushright} \large

\emph{Asessor:} \\

Dr Paul \textsc{Groth}\\

\end{flushright}

\end{minipage}\\[2cm]



%----------------------------------------------------------------------------------------

%	LOGO SECTION

%----------------------------------------------------------------------------------------



% \framebox{\rule{0pt}{2.5cm}\rule{2.5cm}{0pt}}\\[0.5cm]

\includegraphics[width=2.5cm]{data/images/uva.png}\\ % Include a department/university logo - this will require the graphicx package

% \textsc{\large \red{institute name}}\\[1.0cm] % 

 

%----------------------------------------------------------------------------------------



\vfill % Fill the rest of the page with whitespace



\end{titlepage}

\tableofcontents
\newpage

\section*{Abstract}
We generate Knowledge! \cite{kipf_contrastive_2020}

We apply genius idea of having a VAE learn latent features of the data to KGs. Building on successful models of prior work, a linear, a convolutional and a architecture combining both methods are tested on the two most popular KGs. To best possible train out models on sparse graph representation, we implement a permutation invariant loss function.
We compare the performance of our models to sate of the art link predictors, as well as link predictors including the variational module.
The results compare ???
The model $x$ outperfoms the others, indicating that convolutions are [not] necessary.
Interpolation of the latent space shows that the model learns features???
When generating subgraphs with up to $x$ nodes, we see ???
Finally we filter generated triples for predicates which imply the subject or object to be entity of the class location. $x$ out of these triples adhere to this axiom. Thus we can say that VAE's are to a certain extend able to capture the underlying semantics of a KG.   


% \section*{Notes}
% \input{notes}

\section{Introduction}
\input{sections/section1}

\section{Related Work}
\input{sections/section2}

\section{Background}
\input{sections/section3}

\section{Methods}
\input{sections/section4}

\section{Experiments \& Results}
\input{sections/section5}

% \section{Results}
% \input{sections/section6}

\section{Discussion \& Future Work}
\input{sections/section7}

\section*{Ideas}
\input{ideas}

\section*{Open Issues}
\input{issues}

\printbibliography
\end{document}

% \bibliographystyle{unsrt} % We choose the "plain" reference style
% \bibliography{fullrefs} % Entries are in the "refs.bib" file

