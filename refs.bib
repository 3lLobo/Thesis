
@inproceedings{jin_unsupervised_2019,
	address = {Florence, Italy},
	title = {Unsupervised {Learning} of {PCFGs} with {Normalizing} {Flow}},
	url = {https://www.aclweb.org/anthology/P19-1234},
	doi = {10.18653/v1/P19-1234},
	abstract = {Unsupervised PCFG inducers hypothesize sets of compact context-free rules as explanations for sentences. PCFG induction not only provides tools for low-resource languages, but also plays an important role in modeling language acquisition (Bannard et al., 2009; Abend et al. 2017). However, current PCFG induction models, using word tokens as input, are unable to incorporate semantics and morphology into induction, and may encounter issues of sparse vocabulary when facing morphologically rich languages. This paper describes a neural PCFG inducer which employs context embeddings (Peters et al., 2018) in a normalizing flow model (Dinh et al., 2015) to extend PCFG induction to use semantic and morphological information. Linguistically motivated sparsity and categorical distance constraints are imposed on the inducer as regularization. Experiments show that the PCFG induction model with normalizing flow produces grammars with state-of-the-art accuracy on a variety of different languages. Ablation further shows a positive effect of normalizing flow, context embeddings and proposed regularizers.},
	urldate = {2020-05-05},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jin, Lifeng and Doshi-Velez, Finale and Miller, Timothy and Schwartz, Lane and Schuler, William},
	month = jul,
	year = {2019},
	pages = {2442--2452},
	file = {Full Text PDF:C\:\\Users\\fwolf\\Zotero\\storage\\IYM2VFG9\\Jin et al. - 2019 - Unsupervised Learning of PCFGs with Normalizing Fl.pdf:application/pdf}
}

@article{rezende_variational_2016,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1505.05770},
	abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	urldate = {2020-05-05},
	journal = {arXiv:1505.05770 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	month = jun,
	year = {2016},
	note = {arXiv: 1505.05770},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Computation, Statistics - Methodology},
	annote = {Comment: Proceedings of the 32nd International Conference on Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\fwolf\\Zotero\\storage\\ZYV38DAF\\Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\fwolf\\Zotero\\storage\\RK8HUUL2\\1505.html:text/html}
}

@article{yang_feedback_2020,
	title = {Feedback {Recurrent} {AutoEncoder}},
	url = {http://arxiv.org/abs/1911.04018},
	abstract = {In this work, we propose a new recurrent autoencoder architecture, termed Feedback Recurrent AutoEncoder (FRAE), for online compression of sequential data with temporal dependency. The recurrent structure of FRAE is designed to efficiently extract the redundancy along the time dimension and allows a compact discrete representation of the data to be learned. We demonstrate its effectiveness in speech spectrogram compression. Specifically, we show that the FRAE, paired with a powerful neural vocoder, can produce high-quality speech waveforms at a low, fixed bitrate. We further show that by adding a learned prior for the latent space and using an entropy coder, we can achieve an even lower variable bitrate.},
	urldate = {2020-05-07},
	journal = {arXiv:1911.04018 [cs, eess, stat]},
	author = {Yang, Yang and Sautière, Guillaume and Ryu, J. Jon and Cohen, Taco S.},
	month = feb,
	year = {2020},
	note = {arXiv: 1911.04018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\fwolf\\Zotero\\storage\\8X92ZTZG\\Yang et al. - 2020 - Feedback Recurrent AutoEncoder.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\fwolf\\Zotero\\storage\\UJVMX3D7\\1911.html:text/html}
}

@article{kingma_introduction_2019,
	title = {An {Introduction} to {Variational} {Autoencoders}},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {https://www.nowpublishers.com/article/Details/MAL-056},
	doi = {10.1561/2200000056},
	abstract = {An Introduction to Variational Autoencoders},
	language = {English},
	number = {4},
	urldate = {2020-05-07},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Kingma, Diederik P. and Welling, Max},
	month = nov,
	year = {2019},
	note = {Publisher: Now Publishers, Inc.},
	pages = {307--392},
	file = {Full Text PDF:C\:\\Users\\fwolf\\Zotero\\storage\\8I69DBEQ\\Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf:application/pdf;Snapshot:C\:\\Users\\fwolf\\Zotero\\storage\\K2AB4SFQ\\MAL-056.html:text/html}
}

@article{liu_graph_2019,
	title = {Graph {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1905.13177},
	abstract = {We introduce graph normalizing flows: a new, reversible graph neural network model for prediction and generation. On supervised tasks, graph normalizing flows perform similarly to message passing neural networks, but at a significantly reduced memory footprint, allowing them to scale to larger graphs. In the unsupervised case, we combine graph normalizing flows with a novel graph auto-encoder to create a generative model of graph structures. Our model is permutation-invariant, generating entire graphs with a single feed-forward pass, and achieves competitive results with the state-of-the art auto-regressive models, while being better suited to parallel computing architectures.},
	urldate = {2020-05-19},
	journal = {arXiv:1905.13177 [cs, stat]},
	author = {Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
	month = may,
	year = {2019},
	note = {arXiv: 1905.13177},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\fwolf\\Zotero\\storage\\NMCULX3N\\Liu et al. - 2019 - Graph Normalizing Flows.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\fwolf\\Zotero\\storage\\93RKDT75\\1905.html:text/html}
}
